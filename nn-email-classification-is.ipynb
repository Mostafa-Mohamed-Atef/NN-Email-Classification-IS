{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c3085a8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-04T00:42:47.499040Z",
     "iopub.status.busy": "2024-12-04T00:42:47.498665Z",
     "iopub.status.idle": "2024-12-04T00:42:48.885532Z",
     "shell.execute_reply": "2024-12-04T00:42:48.883969Z"
    },
    "papermill": {
     "duration": 1.392779,
     "end_time": "2024-12-04T00:42:48.887389",
     "exception": true,
     "start_time": "2024-12-04T00:42:47.494610",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "BackendError",
     "evalue": "POST failed with: {\"errors\":[\"Unauthenticated\"],\"error\":{\"code\":16,\"details\":[]},\"wasSuccessful\":false}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBackendError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkagglehub\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Download latest version\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mkagglehub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_download\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madvaithsrao/enron-fraud-email-dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath to dataset files:\u001b[39m\u001b[38;5;124m\"\u001b[39m, path)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kagglehub/datasets.py:28\u001b[0m, in \u001b[0;36mdataset_download\u001b[0;34m(handle, path, force_download)\u001b[0m\n\u001b[1;32m     26\u001b[0m h \u001b[38;5;241m=\u001b[39m parse_dataset_handle(handle)\n\u001b[1;32m     27\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;241m.\u001b[39mto_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mEXTRA_CONSOLE_BLOCK})\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kagglehub/registry.py:23\u001b[0m, in \u001b[0;36mMultiImplRegistry.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impls):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mis_supported(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m         fails\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(impl)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kagglehub/kaggle_cache_resolver.py:114\u001b[0m, in \u001b[0;36mDatasetKaggleCacheResolver.__call__\u001b[0;34m(self, h, path, force_download)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m h\u001b[38;5;241m.\u001b[39mis_versioned():\n\u001b[1;32m    112\u001b[0m     dataset_ref[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVersionNumber\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(h\u001b[38;5;241m.\u001b[39mversion)\n\u001b[0;32m--> 114\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mATTACH_DATASOURCE_REQUEST_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasetRef\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_ref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDEFAULT_CONNECT_TIMEOUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mATTACH_DATASOURCE_READ_TIMEOUT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmountSlug\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m    122\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult.mountSlug\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m field missing from response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kagglehub/clients.py:340\u001b[0m, in \u001b[0;36mKaggleJwtClient.post\u001b[0;34m(self, request_name, data, timeout)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwasSuccessful\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    339\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST failed with: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendError(msg)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_response:\n\u001b[1;32m    342\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m field missing from response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mBackendError\u001b[0m: POST failed with: {\"errors\":[\"Unauthenticated\"],\"error\":{\"code\":16,\"details\":[]},\"wasSuccessful\":false}"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"advaithsrao/enron-fraud-email-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a73e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T23:32:10.532480Z",
     "iopub.status.busy": "2024-12-03T23:32:10.531700Z",
     "iopub.status.idle": "2024-12-04T00:26:22.245429Z",
     "shell.execute_reply": "2024-12-04T00:26:22.243921Z",
     "shell.execute_reply.started": "2024-12-03T23:32:10.532441Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(os.path.join(path, 'enron_data_fraud_labeled.csv'))  # Update `path` to your dataset's location\n",
    "df.head()\n",
    "\n",
    "# Step 1: Clean and preprocess the data\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_email(email):\n",
    "    # Check for missing values before processing\n",
    "    if pd.isnull(email):\n",
    "        return \"\"\n",
    "    # Remove non-alphabetic characters and stopwords\n",
    "    email = ' '.join([word for word in email.split() if word.isalpha() and word.lower() not in stop_words])\n",
    "    return email\n",
    "\n",
    "df['cleaned_body'] = df['Body'].apply(preprocess_email)  # Adjust column name if necessary\n",
    "\n",
    "# Encode labels (assuming 'Label' column contains target labels like 'spam' or 'ham')\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['Label'])\n",
    "\n",
    "# Step 2: Tokenization and padding\n",
    "# Define features and target\n",
    "X = df['cleaned_body'].fillna(\"\")  # Replace NaN with empty strings\n",
    "y = df['label_encoded']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize text data\n",
    "tokenizer = Tokenizer(num_words=10000)  # Adjust vocabulary size as needed\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "max_length = 100  # Adjust max length based on your dataset\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Step 3: Build the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=max_length),  # Embedding layer\n",
    "    Bidirectional(LSTM(64, return_sequences=False)),  # LSTM layer\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 4: Train the model\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "    X_train_padded,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=5,  # Adjust epochs based on dataset size\n",
    "    batch_size=32,  # Adjust batch size based on your hardware\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef200f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:37:34.199157Z",
     "iopub.status.busy": "2024-12-04T00:37:34.198729Z",
     "iopub.status.idle": "2024-12-04T00:38:22.260887Z",
     "shell.execute_reply": "2024-12-04T00:38:22.259902Z",
     "shell.execute_reply.started": "2024-12-04T00:37:34.199111Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 5: Evaluate the model\n",
    "print(\"Evaluating the model...\")\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test, verbose=1)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Generate a classification report\n",
    "# Generate a classification report\n",
    "# Ensure target names are strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac0002",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:38:22.263131Z",
     "iopub.status.busy": "2024-12-04T00:38:22.262739Z",
     "iopub.status.idle": "2024-12-04T00:39:10.632742Z",
     "shell.execute_reply": "2024-12-04T00:39:10.631624Z",
     "shell.execute_reply.started": "2024-12-04T00:38:22.263079Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Original class names:\", label_encoder.classes_)\n",
    "class_names = [str(cls) for cls in label_encoder.classes_]  # Ensure class names are strings\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = (model.predict(X_test_padded) > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05deef",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model (optional)\n",
    "model.save('email_classification_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4230881,
     "sourceId": 7294563,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.823153,
   "end_time": "2024-12-04T00:42:49.309066",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-04T00:42:44.485913",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
